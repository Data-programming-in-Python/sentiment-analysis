{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\chris\\anaconda3\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas) (1.17.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\chris\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\chris\\anaconda3\\lib\\site-packages (from sklearn) (0.21.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install sklearn\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from stop_words import stop_words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Vectorized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorized = pd.read_csv('vectorized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test\n",
      "     volum  portray  sabotag  shoddi  clichridden  chang  bloat  compass  \\\n",
      "714      0        0        0       0            0      0      0        0   \n",
      "605      0        0        0       0            0      0      0        0   \n",
      "120      0        0        0       0            0      0      0        0   \n",
      "208      0        0        0       0            0      0      0        0   \n",
      "380      0        0        0       0            0      0      0        0   \n",
      "..     ...      ...      ...     ...          ...    ...    ...      ...   \n",
      "485      0        0        0       0            0      0      0        0   \n",
      "405      0        0        0       0            0      0      0        0   \n",
      "239      0        0        0       0            0      0      0        0   \n",
      "135      0        0        0       0            0      0      0        0   \n",
      "164      0        0        0       0            0      0      0        0   \n",
      "\n",
      "     throwback  desol  ...  uncommon  turkish  ralston  clumsi  tight  \\\n",
      "714          0      0  ...         0        0        0       0      0   \n",
      "605          0      0  ...         0        0        0       0      0   \n",
      "120          0      0  ...         0        0        0       0      0   \n",
      "208          0      0  ...         0        0        0       0      0   \n",
      "380          0      0  ...         0        0        0       0      0   \n",
      "..         ...    ...  ...       ...      ...      ...     ...    ...   \n",
      "485          0      0  ...         0        0        0       0      0   \n",
      "405          0      0  ...         0        0        0       0      0   \n",
      "239          0      0  ...         0        0        0       0      0   \n",
      "135          0      0  ...         0        0        0       0      0   \n",
      "164          0      0  ...         0        0        0       0      0   \n",
      "\n",
      "     unpredict  tiresom  tooth  wisdom  documentarian  \n",
      "714          0        0      0       0              0  \n",
      "605          0        0      0       0              0  \n",
      "120          0        0      0       0              0  \n",
      "208          1        0      0       0              0  \n",
      "380          0        0      0       0              0  \n",
      "..         ...      ...    ...     ...            ...  \n",
      "485          0        0      0       0              0  \n",
      "405          0        0      0       0              0  \n",
      "239          0        0      0       0              0  \n",
      "135          0        0      0       0              0  \n",
      "164          0        0      0       0              0  \n",
      "\n",
      "[258 rows x 1118 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    vectorized, vectorized['_Freshness'], test_size=0.3, random_state=42)\n",
    "\n",
    "print('x_test')\n",
    "print(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.8875968992248062\n",
      "Confusion Matrix [[ 90  29]\n",
      " [  0 139]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = MultinomialNB().fit(x_train, y_train)\n",
    "predicted= clf.predict(x_test)\n",
    "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "\n",
    "print('Confusion Matrix', metrics.confusion_matrix(y_test, predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['clf_model.pkl']"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(clf, 'clf_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model with individual input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaner functions for the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(line): return line.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(line):\n",
    "    ps = PorterStemmer()\n",
    "\n",
    "    words = line.split()\n",
    "    \n",
    "    return_list = [ps.stem(word.strip()) for word in words]\n",
    "\n",
    "    return ' '.join(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(line):\n",
    "\n",
    "    words = line.split()\n",
    "    \n",
    "    kept_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    return ' '.join(kept_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters_and_numbers(line):\n",
    "    return re.sub(r'([^a-zA-Z\\s]+?)', '', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_irrelevant_words():\n",
    "    \n",
    "    # irrelevant words list was generated in a separate script that evaluated whether words were either\n",
    "    # very low occurrence, or were similiarly represented in both positive and negative reviews.\n",
    "    irrelevant_words_file = open('irrelevant_words.txt')    \n",
    "    lines = irrelevant_words_file.readlines()\n",
    "    irrelevant_words_set = {word.strip() for word in lines}\n",
    "    irrelevant_words_file.close()\n",
    "    \n",
    "    return irrelevant_words_set\n",
    "\n",
    "irrelevant_words = get_irrelevant_words()\n",
    "\n",
    "def remove_irrelevant_words(line):\n",
    "    words = line.split()\n",
    "    kept_words = [word for word in words if word not in irrelevant_words]\n",
    "    return ' '.join(kept_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_set(df): \n",
    "    df.dropna()\n",
    "    word_set = set()\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            review_words = row['Review'].split()\n",
    "        except:\n",
    "            continue\n",
    "        for word in review_words:\n",
    "            word = word.strip()\n",
    "            if word not in irrelevant_words:\n",
    "                word_set.add(word)\n",
    "                \n",
    "    return word_set\n",
    "\n",
    "rootcleaned = pd.read_csv('cleaned.csv')\n",
    "wordset = get_words_set(rootcleaned)\n",
    "wordset.add('_Freshness') \n",
    "\n",
    "def create_row_dict(index, row, word_set):\n",
    "    \n",
    "    if index % 10000 == 0:\n",
    "        print('processing index ', index, '.')\n",
    "    \n",
    "    try:\n",
    "        row_words = set(row['Review'].split())\n",
    "    except:\n",
    "        row_words = set()\n",
    "    \n",
    "    return_dict = {header: (0, 1)[header in row_words] for header in word_set}\n",
    "    #return_dict['_Freshness'] = row['Freshness']\n",
    "    return return_dict\n",
    "\n",
    "\n",
    "def vectorize(df):\n",
    "           \n",
    "    dict_list = [create_row_dict(index, row, wordset) for index, row in df.iterrows()]\n",
    "\n",
    "    return_df = pd.DataFrame(dict_list)\n",
    "\n",
    "    print(return_df.head())\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df['Review'] = df['Review'].apply(lower_case)\n",
    "    #print('Finished, lower_case: ')\n",
    "    #get_time()\n",
    "    df['Review'] = df['Review'].apply(remove_stop_words)\n",
    "    #print('Finished, remove_stop_words: ')\n",
    "    #get_time()\n",
    "    df['Review'] = df['Review'].apply(remove_special_characters_and_numbers)\n",
    "    #print('Finished, remove_special_characters_and_numbers: ')\n",
    "    #get_time()\n",
    "    df['Review'] = df['Review'].apply(stem_words)\n",
    "    #print('Finished, stem_words: ')\n",
    "    #get_time()\n",
    "    \n",
    "    df['Review'] = df['Review'].apply(remove_irrelevant_words)\n",
    "    #print('Finished, remove_irrelevant_words: ')\n",
    "    #get_time()\n",
    "    \n",
    "    df['Review'].replace('', np.nan, inplace=True)\n",
    "    df.dropna(subset=['Review'], inplace=True)\n",
    "    return df\n",
    "\n",
    "    #df.to_csv('cleaned.csv', index=False)\n",
    "    #print('Finished, cleaned to csv: ')\n",
    "    #get_time()\n",
    "    \n",
    "#raw_df = pd.read_csv('truncated.csv')\n",
    "# raw_df = pd.read_csv('rotten_tomatoes_reviews.csv')\n",
    "\n",
    "#clean_data(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_text(in_df):\n",
    "    # This method will get the data in the correct format for testing the model\n",
    "    cd = clean_data(in_df)\n",
    "    vectorized = vectorize(cd)\n",
    "    return vectorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTest = pd.DataFrame()\n",
    "\n",
    "dfTest = dfTest.append({\"Review\":\"fail\"}, ignore_index=True)\n",
    "##stringTestNegative = \"This is a test string to see how terrible our model does when guessing a negative outcome\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the data before passing it into the model.  This will remove stop words, take out stubs and put the text into numerical values for the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_set size:  1118\n",
      "processing index  0 .\n",
      "   bittersweet  centuri  unnecessari  invent  exhilar  chadwick  limp  \\\n",
      "0            0        0            0       0        0         0     0   \n",
      "\n",
      "   blackandwhit  tens  savour  ...  explor  utmost  loneli  meticul  classic  \\\n",
      "0             0     0       0  ...       0       0       0        0        0   \n",
      "\n",
      "   lurch  frothi  stir  mirthless  societi  \n",
      "0      0       0     0          0        0  \n",
      "\n",
      "[1 rows x 1118 columns]\n"
     ]
    }
   ],
   "source": [
    "scrubbedInputTest = prepare_input_text(dfTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scrubbedInputTest.to_csv('scrubbedinput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = clf.predict(scrubbedInputTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
